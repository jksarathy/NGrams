#Jayshree Sarathy
#jayshree.sarathy@yale.edu
#Using Python 3

import sys
import random
import string
import math
from collections import defaultdict

#generate word using unsmoothed trigram
def generate_triword():
    word = "# #"
    current = ' '
    context = ("#","#")
    while current is not '#':
        if current == ' ': context = ('#','#')
        #generate randomly weighted phoneme based on phoneme pair
        current = generate_triphenome(context)
        context = (context[1],current)
        word += " " + current
    return word

#generate word using unsmoothed bigram
def generate_biword():
    word = "#"
    current = ' '
    while current is not '#':
        if current == ' ': current = '#'
        #generate randomly weighted phoneme based on current phoneme
        current = generate_biphenome(current)
        word += " " + current
    return word

#generate phoneme using unsmoothed trigram
def generate_triphenome(pair):
    #generate random number between 0 and 1
    rand = random.uniform(0,1)
    #find correct bucket and return phoneme
    for p in ngram[pair]:
        rand = rand - ngram[pair][p]
        if rand < 0.0: return p
    return p

#generate phoneme using unsmoothed bigram
def generate_biphenome(phon):
    #generate random number between 0 and 1
    rand = random.uniform(0,1)
    #find correct bucket and return phoneme
    for p in ngram[phon]:
        rand = rand - ngram[phon][p]
        if rand < 0.0: return p
    return p

#calculate unsmoothed probabilities
def calculateProbabilities():
    for given in counts:
        for predict in ncounts[given]:
            ngram[ given ][ predict ] = float( ncounts [given] [predict]) / float( counts [given])

#calculate smoothed probabilities
def calculateSmoothedProbabilities():
    if n == 2:
        #loop through all seen phonemes
        for given in seen:
            for predict in seen:
                ngram[ given ][ predict ] = float( ncounts [given] [predict] + smoothing) / float( counts [given] + smoothing*len(seen) )

    elif n == 3:
        seenPairs = []
        #create list of all possible pairs of seen phonemes
        for phen1 in seen:
            for phen2 in seen:
                seenPairs.append( (phen1,phen2) )
        #loop through all seen phoneme pairs and phonemes
        for given in seenPairs:
            for predict in seen:
                #factor in smoothing to numerator and denominator
                ngram[ given][ predict] = float( ncounts[given] [predict] + smoothing) / float( counts[given] + smoothing*len(seen) )

#print words generated by unsmoothed bigrams and trigrams
def printNewWords():
    for j in range(25):
        if n == 2: print(generate_biword())
        elif n == 3: print(generate_triword())

#calculate and print probabilties for each word and perplexity of corpus
def printPerplexity(test_file):
    log_sum = 0
    count = 0
    
    for line in test_file:
        #running sum of phoneme log probabilities
        log_prob = 0
        line = "# " + line.strip('\n') + " #"
        phonemes = line.split()

        if n == 2:
            for i in range(len(phonemes) - 1):
                x = ngram[phonemes[i]][phonemes[i+1]]
                log_prob += math.log( x, 2)
                count+=1
        elif n == 3:
            for i in range(len(phonemes) - 2):
                pair = (phonemes[i], phonemes[i+1])
                log_prob += math.log( ngram [pair] [phonemes[i+2]], 2)
                count+=1

        #add word log probability to running log sum
        log_sum += log_prob
        #convert back from log to normal probability
        probability = 2**(log_prob)
        print(line.strip('# ') + '\t' + str(probability))

    #calculate perplexity using log sum
    perplexity = 2**(log_sum * ( (-1.0) / count ))
    print( str(perplexity))

#open test file
language = open(sys.argv[1])
#record n in n-gram
n = int(sys.argv[2])

#dictionary of all seen phonemes
seen = []
#dictionaries for calculating conditional probabilities of phonemes and phoneme pairs
counts = defaultdict( lambda:0)
ncounts = defaultdict( lambda:defaultdict (lambda: 0))

#go through phonemes in test file and add them to dictionaries
if n == 2:
    for line in language:
        line = line.replace('\'','')
        line = "#" + line.strip(string.lowercase) + " #"
        phonemes = line.split()
        #update seen phonemes without duplicates
        seen = seen + list(set(phonemes) - set(seen))

        #add to count of each phoneme
        for i in range(len(phonemes) - 1):
            counts [phonemes [i] ] = counts [phonemes [i] ] + 1
            ncounts[phonemes [i] ] [phonemes [i + 1] ] = ncounts[ phonemes[i] ][ phonemes [i+1] ] + 1

elif n == 3:
    for line in language:
        line = line.replace('\'','')
        line = "# #" + line.strip(string.lowercase) + " #"
        phonemes = line.split()
        #update seen phonemes without duplicates
        seen = seen + list(set(phonemes) - set(seen))

        #add to count of each phoneme
        for i in range (len (phonemes) - 2):
            pair = (phonemes [i], phonemes [i+1])
            counts [pair] = counts [pair] + 1;
            ncounts [pair] [phonemes [i+2]] = ncounts [pair] [phonemes [i + 2]] + 1

#close file
language.close()
#default dictionary for ngrams
ngram = defaultdict(lambda:{})

#check if there are 3 arguments
if len(sys.argv) == 4:
    #add-1 smoothing
    smoothing = 1
    test_file = open(sys.argv[3])
    calculateSmoothedProbabilities()
    #calculate perplexity of given test file
    printPerplexity(test_file)
    test_file.close()

#otherwise do part 1 of assignment
else:
    calculateProbabilities()
    printNewWords()


            
            
